#Set up 
```{r}
library(tidyverse)
library(caret)
library(reticulate)
library(recipes)
library(vip)
library(rpart)
library(beepr)
library(cutpointr)
library(gt)

#Read in data
elon <- read.csv("elon_clean.csv")
train <- read.csv("tweet_train.csv")
test <- read.csv("tweet_test.csv")

#Drop ID
train <- train[-c(1)] 
test <- test[-c(1)] 
```

#Load in encoding model
```{r}
use_condaenv('r-reticulate')

#conda_install(envname  = 'r-reticulate',
#              packages = 'sentence_transformers',
#              pip      = TRUE)

st <- import('sentence_transformers')

model.name <- 'bert-base-uncased'

longformer      <- st$models$Transformer(model.name)
pooling_model   <- st$models$Pooling(longformer$get_word_embedding_dimension())
LFmodel         <- st$SentenceTransformer(modules = list(longformer,pooling_model))

```

#Recipe and folds 
```{r}
#Create recipe
blueprint <- recipe(x = train,vars = colnames(train), 
                    roles = c('outcome', rep('predictor',774))) %>% 
  step_dummy('month', one_hot = T) %>% 
  step_harmonic('day',frequency=1,cycle_size=7, role='predictor') %>%
  step_harmonic('date',frequency=1,cycle_size=31,role='predictor') %>%
  step_harmonic('hour',frequency=1,cycle_size=24,role='predictor') %>%
  step_normalize(paste0('V',1:768)) %>%
  step_normalize(c('day_sin_1','day_cos_1',
                   'date_sin_1','date_cos_1',
                   'hour_sin_1','hour_cos_1')) %>%
  step_normalize(all_numeric_predictors())

#Create fold 
folds = cut(seq(1, nrow(train)), breaks = 10, labels = F)

#Index list
index <-vector('list', 10)

for (i in 1:10){
  index[[i]] <- which(folds != i)
}



cv <- trainControl(method = "cv",
                   index  = index,
                   classProbs = TRUE)


#Hyperparameter grid 
grid <- data.frame(alpha = 0 , 
               lambda = seq(0,3, 0.01))

beepr::beep(4)
```

#GLM
```{r}
set.seed(0294875)
model_glmnet <- caret::train(blueprint, 
                             data = train,
                             method= "glmnet",
                             trainControl = cv,
                             family = 'binomial',
                             tuneGrid = grid)

prediction_glm <- predict(model_glmnet, test, type = 'prob')

#Best fit 
model_glmnet$bestTune
model_glmnet$results[29,]

vip(model_glmnet)

plot(model_glmnet)

saveRDS(model_glmnet,"./GLM_Model")
```

#GLM accuracy 
```{r}
#Confusion matrix
glm_pred_class <- ifelse(prediction_glm$positive>.6, 1,0)
glm_confusion <- table(test$sentiment,
                      glm_pred_class)
  #AUC
glm_cut <- cutpointr(x = prediction_glm$positive,
                 class = test$sentiment)
glm_auc <- auc(glm_cut) 
  #ACC
glm_acc <-model_glmnet$results[29,][3]
glm_rsq <- glm_acc^2
  #TPR
glm_tpr <- glm_confusion[2,2]/
  (glm_confusion[2,1]+
     glm_confusion[2,2])
  #TNR
glm_tnr <- glm_confusion[1,1]/
  (glm_confusion[1,1] + 
     glm_confusion[1,2])

  #PRE
glm_pre <- glm_confusion[2,2]/
  (glm_confusion[1,2] +
     glm_confusion[2,2])
```

#Decision tree model 
```{r}
set.seed(0294875)
cv_tree <- trainControl(method = "cv",
                   index  = index,
                   classProbs = F)

grid_tree <- data.frame(cp=seq(0,0.02,.001)) #Hyper parameter: complexity parameter




model_tree <- caret::train(blueprint,
                             data      = train,
                             method    = 'rpart',
                             tuneGrid  = grid_tree,
                             trControl = cv_tree,
                             control   = list(minsplit=20,
                                             minbucket = 2,
                                             maxdepth = 30))

model_tree$bestTune
model_tree$results[4,]

predict_tree <- predict(model_tree, test,type = 'prob')
predict_tree

saveRDS(model_tree,"./Tree_Model")

```

#Tree accuracy 
```{r}
#Confusion matrix
tree_pred_class <- ifelse(predict_tree$positive>.6, 1,0)
tree_confusion <- table(test$sentiment,
                      tree_pred_class)
  #AUC
tree_cut <- cutpointr(x = predict_tree$positive,
                 class = test$sentiment)
tree_auc <- auc(glm_cut) 
  #ACC
tree_acc <-model_tree$results[4,][2]
tree_rsq <- tree_acc^2
  #TPR
tree_tpr <- tree_confusion[2,2]/
  (tree_confusion[2,1]+
     tree_confusion[2,2])
  #TNR
tree_tnr <- tree_confusion[1,1]/
  (tree_confusion[1,1] + 
     tree_confusion[1,2])

  #PRE
tree_pre <- tree_confusion[2,2]/
  (tree_confusion[1,2] +
     tree_confusion[2,2])
```

#Gradient boosting forest
```{r}
#Grid
grid_forest <- expand.grid(shrinkage = 0.1,
                    n.trees = 1:500,
                    interaction.depth = 5,
                    n.minobsinnode = 10) 

model_gbm <- caret::train(blueprint,
                             data      = train,
                             method    = 'gbm',
                             tuneGrid  = grid_forest,
                             trControl = cv,
                             bag.fraction = 1,
                             verbose = F)
#Optimized parameters
model_gbm$bestTune
model_gbm$results[399,] #399 iteration

#Predict against test data
predict_gbm <- predict(model_gbm, test,type = 'prob')

saveRDS(model_gbm,"./GBM_model")
```

#GBM accuracy 
```{r}
#Confusion matrix
gbm_pred_class <- ifelse(predict_gbm$positive>.6, 1,0)
gbm_confusion <- table(test$sentiment,
                      gbm_pred_class)
  #AUC
gbm_cut <- cutpointr(x = gbm_pred_class,
                 class = test$sentiment)
gbm_auc <- auc(gbm_cut) 
  #ACC
gbm_acc <-model_gbm$results[399,][5]
gbm_rsq <- gbm_acc^2
  #TPR
gbm_tpr <- gbm_confusion[2,2]/
  (gbm_confusion[2,1]+
     gbm_confusion[2,2])
  #TNR
gbm_tnr <- gbm_confusion[1,1]/
  (gbm_confusion[1,1] + 
     gbm_confusion[1,2])

  #PRE
gbm_pre <- gbm_confusion[2,2]/
  (gbm_confusion[1,2] +
     gbm_confusion[2,2])
```

#Compare
```{r}
df <- tibble(auc = c(glm_auc, tree_auc, gbm_auc),
                    acc = c(glm_acc$Accuracy,tree_acc$Accuracy,gbm_acc$Accuracy),
                    tpr = c(glm_tpr,tree_tpr,gbm_tpr),
                    tnr = c(glm_tnr,tree_tnr, gbm_tnr),
                    pre = c(glm_pre,tree_pre,gbm_pre),
                    Rsq = c(glm_rsq, tree_rsq, gbm_rsq))
rownames(df) <- c("GLM", "Tree", "GBM")
tab <- df %>% gt( rownames_to_stub = T) %>% 
  tab_header("Machine learning model comparison",
             subtitle = "General linear model,
             Forest model, and Gradient boosted foest model")
#gtsave(tab,"table.html")

plot(model_glmnet)
plot(model_tree)
plot(model_gbm)
vip(model_glmnet,num_features = 10, geom = "point") + theme_dark()
vip(model_tree,num_features = 10, geom = "point") + theme_dark()
vip(model_gbm,num_features = 10, geom = "point") + theme_dark()


```

